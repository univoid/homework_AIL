{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 課題. MNISTデータセットを多層パーセプトロン(MLP)で学習せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意\n",
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データはtrain_X, train_y, テストデータはtest_Xで与えられます\n",
    "    - train_Xとtrain_yをtrain_X, train_yとvalid_X, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - test_Xに対して予想ラベルpred_yを作り, homework関数の戻り値としてください\\\n",
    "- pred_yのtest_yに対する精度(F値)で評価します\n",
    "- 全体の実行時間がiLect上で60分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください (必要なものは全てhomework関数に入れてください)\n",
    "- 解答提出時には Answer Cell の内容のみを提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLPの実装にTensorflowなどのライブラリを使わないでください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ヒント\n",
    "- 出力yはone-of-k表現\n",
    "- 最終層の活性化関数はソフトマックス関数, 誤差関数は多クラス交差エントロピー\n",
    "- 最終層のデルタは教科書参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルのhomework関数を完成させて提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "ilect": {
     "is_homework": true
    }
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "   \n",
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def deriv_sigmoid(x):\n",
    "        return sigmoid(x)*(1 - sigmoid(x))\n",
    "    \n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x/np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def deriv_softmax(x):\n",
    "        return softmax(x)*(1 - softmax(x))\n",
    "    \n",
    "    def oneHotEnding(train_y):\n",
    "        y_size = len(train_y)\n",
    "        y = np.zeros((y_size, 10))\n",
    "        y[np.arange(y_size), train_y] = 1\n",
    "        return y\n",
    "    \n",
    "    def train(x, t, eps=0.2):\n",
    "        nonlocal W1, b1, W2, b2\n",
    "        \n",
    "        # Forward Propagation Layer1\n",
    "        u1 = np.matmul(x, W1) + b1\n",
    "        z1 = sigmoid(u1)\n",
    "    \n",
    "        # Forward Propagation Layer2\n",
    "        u2 = np.matmul(z1, W2) + b2\n",
    "        z2 = softmax(u2)\n",
    "    \n",
    "        # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "        y = z2\n",
    "        cost = np.sum(-t*np.log(y))\n",
    "        delta_2 = y - t\n",
    "        delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T)\n",
    "\n",
    "        # Update Parameters Layer1\n",
    "        dW1 = np.matmul(x.T, delta_1)\n",
    "        db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "        W1 = W1 - eps*dW1\n",
    "        b1 = b1 - eps*db1\n",
    "    \n",
    "        # Update Parameters Layer2\n",
    "        dW2 = np.matmul(z1.T, delta_2)\n",
    "        db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "        W2 = W2 - eps*dW2\n",
    "        b2 = b2 - eps*db2\n",
    "\n",
    "        return cost\n",
    "    \n",
    "    def test(x):\n",
    "        nonlocal W1, b1, W2, b2\n",
    "        # Forward Propagation Layer1\n",
    "        u1 = np.matmul(x, W1) + b1\n",
    "        z1 = sigmoid(u1)\n",
    "    \n",
    "        # Forward Propagation Layer2\n",
    "        u2 = np.matmul(z1, W2) + b2\n",
    "        z2 = softmax(u2)\n",
    "    \n",
    "        y = z2\n",
    "        return y\n",
    "    \n",
    "\n",
    "    # initialize\n",
    "    # OneHotEncoding\n",
    "    train_y = oneHotEnding(train_y)\n",
    "    # normalize train array\n",
    "    norm = np.linalg.norm(train_X, ord=2, axis=1)\n",
    "    train_X = train_X / norm[:, np.newaxis]\n",
    "    # normalize test array\n",
    "    norm = np.linalg.norm(test_X, ord=2, axis=1)\n",
    "    test_X = test_X / norm[:, np.newaxis]\n",
    "    # Layer1 weights\n",
    "    W1 = np.random.uniform(low=-0.08, high=0.08, size=(784, 32)).astype('float32')\n",
    "    b1 = np.zeros(32).astype('float32')\n",
    "    # Layer2 weights\n",
    "    W2 = np.random.uniform(low=-0.08, high=0.08, size=(32, 10)).astype('float32')\n",
    "    b2 = np.zeros(10).astype('float32')\n",
    "    \n",
    "    # Epoch\n",
    "    for epoch in range(42):\n",
    "        for x, y in zip(train_X, train_y):\n",
    "            cost = train(x[np.newaxis, :], y[np.newaxis, :])\n",
    "            # print(cost)\n",
    "    \n",
    "    pred_y = test(test_X)\n",
    "    # unOneHotEncoding\n",
    "    pred_y = [np.argmax(y) for y in pred_y]\n",
    "    \n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以下のvalidate_homework関数を用いてエラーが起きないか動作確認をして下さい。\n",
    "- 提出に際して、score_homework関数で60分で実行が終わることを確認して下さい。\n",
    "- 評価は以下のscore_homework関数で行われますが、random_stateの値は変更されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checker Cell (for student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "ilect": {
     "course_id": 4,
     "course_rank": 4,
     "is_evaluation": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                               mnist.target.astype('int32'), random_state=42)\n",
    "\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    return train_test_split(mnist_X, mnist_y,\n",
    "                test_size=0.2,\n",
    "                random_state=42)\n",
    "\n",
    "def validate_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "\n",
    "    # validate for small dataset\n",
    "    train_X_mini = train_X[:100]\n",
    "    train_y_mini = train_y[:100]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    print(f1_score(test_y_mini, pred_y, average='macro'))\n",
    "\n",
    "def score_homework():\n",
    "    train_X, test_X, train_y, test_y = load_mnist()\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    print(f1_score(test_y, pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_homework()\n",
    "# score_homework()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
